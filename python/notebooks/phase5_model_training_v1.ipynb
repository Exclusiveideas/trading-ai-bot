{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 5: Model Training V1b â€” Multi-Pattern Binary Classifier (V2)\n\n**Goal:** Train XGBoost to predict win/loss for 2R trades across 5 pattern types, 20 pairs, 4 timeframes\n\n**Dataset:** ~98K labeled patterns with multi-timeframe features\n\n**Previous V1b:** 1,194 labels across 3 pattern types and 6 pairs. This V2 has 80x more data, 5 pattern types, 4 timeframes, and HTF context."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Load & Clean Data\nimport pandas as pd\nimport numpy as np\n\nCSV_PATH = \"/Users/muftau/Documents/programming/trading-ai/python/data/training-all.csv\"\n\ndf_raw = pd.read_csv(CSV_PATH, low_memory=False)\nprint(f\"Loaded {len(df_raw)} rows, {len(df_raw.columns)} columns\")\nprint(f\"Outcome distribution:\\n{df_raw.outcome.value_counts()}\")\n\n# Filter out \"pending\" outcomes\ndf_raw = df_raw[df_raw[\"outcome\"].isin([\"win\", \"loss\"])].reset_index(drop=True)\nprint(f\"\\nAfter filtering pending: {len(df_raw)} rows\")\n\ndf = df_raw.copy()\n\n# --- Drop always-null columns ---\nnull_pct = df.isnull().mean()\nalways_null = null_pct[null_pct > 0.99].index.tolist()\nif always_null:\n    print(f\"Dropping always-null columns: {always_null}\")\n    df = df.drop(columns=always_null)\n\n# --- Separate metadata, target, and analysis-only columns ---\nmetadata_cols = [\n    \"id\", \"pair\", \"start_timestamp\", \"end_timestamp\",\n    \"entry_price\", \"stop_loss\", \"take_profit\", \"notes\",\n    \"nearest_round_number\", \"nearest_support\", \"nearest_resistance\",\n]\ntarget_col = \"outcome\"\nanalysis_only_cols = [\"r_multiple\", \"bars_to_outcome\", \"max_favorable_excursion\"]\n\n# --- Encode target ---\ndf[\"target\"] = (df[\"outcome\"] == \"win\").astype(int)\n\n# --- Encode categoricals ---\ncategorical_cols = [\"pattern_type\", \"timeframe\", \"trend_state\", \"trading_session\", \"rsi_zone\"]\n# Also encode HTF trend states\nhtf_trend_cols = [c for c in df.columns if c.endswith(\"_trend_state\") and c.startswith(\"htf_\")]\ncategorical_cols.extend(htf_trend_cols)\n\nfor col in categorical_cols:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"unknown\")\n\ndf_raw[\"trend_state\"] = df_raw[\"trend_state\"].fillna(\"unknown\")\ndf_raw[\"trading_session\"] = df_raw[\"trading_session\"].fillna(\"unknown\")\n\n# One-hot encode all categoricals\ndf = pd.get_dummies(df, columns=[c for c in categorical_cols if c in df.columns], dtype=int)\n\n# --- Build feature matrix ---\ndrop_cols = metadata_cols + [target_col] + analysis_only_cols + [\"target\", \"quality_rating\"]\nfeature_cols = [c for c in df.columns if c not in drop_cols]\n\nX = df[feature_cols]\ny = df[\"target\"]\n\nprint(f\"\\nFeature matrix: {X.shape}\")\nprint(f\"Features ({len(feature_cols)})\")\nprint(f\"\\nTarget: {y.value_counts().to_dict()}\")\nprint(f\"\\nNull counts in features (top 10):\")\nnull_counts = X.isnull().sum()\nprint(null_counts[null_counts > 0].sort_values(ascending=False).head(10))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Time-Based Train/Test Split (80/20)\nsplit_idx = int(len(X) * 0.8)\n\nX_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n\ntrain_wins = y_train.sum()\ntrain_losses = len(y_train) - train_wins\n\nprint(f\"Train: {len(X_train)} rows ({train_losses} loss, {train_wins} win)\")\nprint(f\"Test:  {len(X_test)} rows ({len(y_test) - y_test.sum()} loss, {y_test.sum()} win)\")\nprint(f\"\\nTrain win rate: {train_wins / len(y_train):.1%}\")\nprint(f\"Test win rate:  {y_test.sum() / len(y_test):.1%}\")\nprint(f\"\\nscale_pos_weight: {train_losses / train_wins:.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Train XGBoost V1b & Evaluate\nimport xgboost as xgb\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n)\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams[\"figure.dpi\"] = 120\n\nspw = (len(y_train) - y_train.sum()) / y_train.sum()\n\nmodel = xgb.XGBClassifier(\n    n_estimators=300,\n    max_depth=6,\n    learning_rate=0.05,\n    scale_pos_weight=spw,\n    eval_metric=\"logloss\",\n    random_state=42,\n    enable_categorical=False,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    min_child_weight=5,\n)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)[:, 1]\n\nprint(\"=== V1b Model Performance ===\")\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\nprint(f\"Precision: {precision_score(y_test, y_pred, zero_division=0):.3f}\")\nprint(f\"Recall:    {recall_score(y_test, y_pred, zero_division=0):.3f}\")\nprint(f\"F1 Score:  {f1_score(y_test, y_pred, zero_division=0):.3f}\")\nprint(f\"AUC-ROC:   {roc_auc_score(y_test, y_prob):.3f}\")\nprint(f\"\\nBaseline (always predict majority): {max(y_test.mean(), 1 - y_test.mean()):.3f}\")\nprint(f\"\\n{classification_report(y_test, y_pred, target_names=['loss', 'win'], zero_division=0)}\")\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(5, 4))\nim = ax.imshow(cm, cmap=\"Blues\")\nax.set_xticks([0, 1])\nax.set_yticks([0, 1])\nax.set_xticklabels([\"loss\", \"win\"])\nax.set_yticklabels([\"loss\", \"win\"])\nax.set_xlabel(\"Predicted\")\nax.set_ylabel(\"Actual\")\nax.set_title(\"Confusion Matrix\")\nfor i in range(2):\n    for j in range(2):\n        ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\",\n                color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\", fontsize=16)\nplt.colorbar(im)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Importance (Task 5.3)\n",
    "importances = model.get_booster().get_score(importance_type=\"gain\")\n",
    "imp_df = pd.DataFrame(\n",
    "    {\"feature\": importances.keys(), \"importance\": importances.values()}\n",
    ").sort_values(\"importance\", ascending=True)\n",
    "\n",
    "top_n = min(15, len(imp_df))\n",
    "top = imp_df.tail(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.barh(top[\"feature\"], top[\"importance\"])\n",
    "ax.set_xlabel(\"Importance (Gain)\")\n",
    "ax.set_title(f\"Top {top_n} Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop {top_n} features:\")\n",
    "for _, row in imp_df.tail(top_n).iloc[::-1].iterrows():\n",
    "    print(f\"  {row['feature']:30s} {row['importance']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: MFE Distribution Analysis (Task 5.3)\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nwins_mfe = df_raw[df_raw[\"outcome\"] == \"win\"][\"max_favorable_excursion\"]\nlosses_mfe = df_raw[df_raw[\"outcome\"] == \"loss\"][\"max_favorable_excursion\"]\n\n# 5a: MFE histogram by outcome\nax = axes[0]\nax.hist(losses_mfe, bins=20, alpha=0.6, label=f\"Loss (n={len(losses_mfe)})\")\nax.hist(wins_mfe, bins=20, alpha=0.6, label=f\"Win (n={len(wins_mfe)})\")\nax.axvline(x=2.0, color=\"red\", linestyle=\"--\", label=\"2R target\")\nax.set_xlabel(\"MFE (R units)\")\nax.set_ylabel(\"Count\")\nax.set_title(\"MFE Distribution by Outcome\")\nax.legend()\n\n# 5b: MFE by quality rating\nax = axes[1]\nquality_groups = df_raw.groupby(\"quality_rating\")[\"max_favorable_excursion\"]\nlabels_q = sorted(df_raw[\"quality_rating\"].dropna().unique())\ndata_q = [quality_groups.get_group(q).dropna().values for q in labels_q]\nbp = ax.boxplot(data_q, tick_labels=[str(int(q)) for q in labels_q], patch_artist=True)\nax.set_xlabel(\"Quality Rating\")\nax.set_ylabel(\"MFE (R units)\")\nax.set_title(\"MFE by Quality Rating\")\n\n# 5c: MFE by trend state\nax = axes[2]\ntrend_groups = df_raw.groupby(\"trend_state\")[\"max_favorable_excursion\"]\nlabels_t = sorted(df_raw[\"trend_state\"].unique())\ndata_t = [trend_groups.get_group(t).dropna().values for t in labels_t if len(trend_groups.get_group(t).dropna()) > 0]\nlabels_t = [t for t in labels_t if len(trend_groups.get_group(t).dropna()) > 0]\nbp2 = ax.boxplot(data_t, tick_labels=labels_t, patch_artist=True)\nax.tick_params(axis=\"x\", rotation=45)\nax.set_xlabel(\"Trend State\")\nax.set_ylabel(\"MFE (R units)\")\nax.set_title(\"MFE by Trend State\")\n\nplt.tight_layout()\nplt.show()\n\n# Key MFE stats\nprint(\"=== MFE Summary ===\")\nprint(f\"Win  MFE: mean={wins_mfe.mean():.2f}R, median={wins_mfe.median():.2f}R\")\nprint(f\"Loss MFE: mean={losses_mfe.mean():.2f}R, median={losses_mfe.median():.2f}R\")\nprint(f\"\\nLosses with MFE >= 1R: {(losses_mfe >= 1.0).sum()} / {len(losses_mfe)} ({(losses_mfe >= 1.0).mean():.1%})\")\nprint(\"  ^ These are trades that moved favorably 1R+ but didn't reach 2R -- V2/V3 could capture these.\")\nprint(f\"\\nLosses with MFE >= 1.5R: {(losses_mfe >= 1.5).sum()} / {len(losses_mfe)} ({(losses_mfe >= 1.5).mean():.1%})\")\nprint(\"  ^ Trades that nearly hit 2R -- strong case for dynamic TP.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Context Performance Analysis\ndf_test_analysis = df_raw.iloc[split_idx:].copy()\ndf_test_analysis[\"predicted\"] = y_pred\ndf_test_analysis[\"predicted_prob\"] = y_prob\ndf_test_analysis[\"is_win\"] = (df_test_analysis[\"outcome\"] == \"win\").astype(int)\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 6a: Win rate by trend state\nax = axes[0, 0]\nts_stats = df_test_analysis.groupby(\"trend_state\").agg(\n    win_rate=(\"is_win\", \"mean\"),\n    count=(\"is_win\", \"count\"),\n)\nts_stats = ts_stats[ts_stats[\"count\"] >= 10].sort_values(\"win_rate\", ascending=True)\nbars = ax.barh(ts_stats.index, ts_stats[\"win_rate\"])\nfor bar, (_, row) in zip(bars, ts_stats.iterrows()):\n    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2,\n            f\"n={int(row['count'])}\", va=\"center\", fontsize=9)\nax.set_xlabel(\"Win Rate\")\nax.set_title(\"Win Rate by Trend State (Test Set)\")\nax.set_xlim(0, 1)\n\n# 6b: Win rate by session\nax = axes[0, 1]\nsess_stats = df_test_analysis.groupby(\"trading_session\").agg(\n    win_rate=(\"is_win\", \"mean\"),\n    count=(\"is_win\", \"count\"),\n)\nsess_stats = sess_stats[sess_stats[\"count\"] >= 10].sort_values(\"win_rate\", ascending=True)\nbars = ax.barh(sess_stats.index, sess_stats[\"win_rate\"])\nfor bar, (_, row) in zip(bars, sess_stats.iterrows()):\n    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2,\n            f\"n={int(row['count'])}\", va=\"center\", fontsize=9)\nax.set_xlabel(\"Win Rate\")\nax.set_title(\"Win Rate by Session (Test Set)\")\nax.set_xlim(0, 1)\n\n# 6c: Win rate by timeframe\nax = axes[1, 0]\ntf_stats = df_test_analysis.groupby(\"timeframe\").agg(\n    win_rate=(\"is_win\", \"mean\"),\n    count=(\"is_win\", \"count\"),\n)\ntf_stats = tf_stats.sort_values(\"win_rate\", ascending=True)\nbars = ax.barh(tf_stats.index, tf_stats[\"win_rate\"])\nfor bar, (_, row) in zip(bars, tf_stats.iterrows()):\n    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2,\n            f\"n={int(row['count'])}\", va=\"center\", fontsize=9)\nax.set_xlabel(\"Win Rate\")\nax.set_title(\"Win Rate by Timeframe (Test Set)\")\nax.set_xlim(0, 1)\n\n# 6d: Calibration\nax = axes[1, 1]\ndf_test_analysis[\"prob_bucket\"] = pd.cut(df_test_analysis[\"predicted_prob\"], bins=10)\ncal = df_test_analysis.groupby(\"prob_bucket\", observed=True).agg(\n    mean_pred=(\"predicted_prob\", \"mean\"),\n    actual_rate=(\"is_win\", \"mean\"),\n    count=(\"is_win\", \"count\"),\n)\ncal = cal[cal[\"count\"] >= 10]\nif len(cal) >= 2:\n    ax.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Perfect calibration\")\n    ax.scatter(cal[\"mean_pred\"], cal[\"actual_rate\"], s=cal[\"count\"] * 2, zorder=5)\n    for _, row in cal.iterrows():\n        ax.annotate(f\"n={int(row['count'])}\",\n                    (row[\"mean_pred\"], row[\"actual_rate\"]),\n                    textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n    ax.set_xlabel(\"Mean Predicted Probability\")\n    ax.set_ylabel(\"Actual Win Rate\")\n    ax.set_title(\"Calibration Plot\")\n    ax.legend()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# Full context breakdown\nprint(\"=== Context Performance (Full Dataset) ===\")\ndf_full = df_raw.copy()\ndf_full[\"is_win\"] = (df_full[\"outcome\"] == \"win\").astype(int)\n\nprint(\"\\n--- By Pattern Type ---\")\nfor pt in sorted(df_full[\"pattern_type\"].unique()):\n    sub = df_full[df_full[\"pattern_type\"] == pt]\n    mfe = sub[\"max_favorable_excursion\"].mean()\n    print(f\"  {pt:25s}  n={len(sub):5d}  win_rate={sub['is_win'].mean():.1%}  avg_mfe={mfe:.2f}R\")\n\nprint(\"\\n--- By Timeframe ---\")\nfor tf in [\"D\", \"H4\", \"H1\", \"M15\"]:\n    sub = df_full[df_full[\"timeframe\"] == tf]\n    if len(sub) == 0:\n        continue\n    mfe = sub[\"max_favorable_excursion\"].mean()\n    print(f\"  {tf:25s}  n={len(sub):5d}  win_rate={sub['is_win'].mean():.1%}  avg_mfe={mfe:.2f}R\")\n\nprint(\"\\n--- By Session ---\")\nfor sess in sorted(df_full[\"trading_session\"].dropna().unique()):\n    sub = df_full[df_full[\"trading_session\"] == sess]\n    mfe = sub[\"max_favorable_excursion\"].mean()\n    print(f\"  {sess:25s}  n={len(sub):5d}  win_rate={sub['is_win'].mean():.1%}  avg_mfe={mfe:.2f}R\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Save Model & Summary\nimport os\nimport json\n\nmodel_dir = \"/Users/muftau/Documents/programming/trading-ai/python/models\"\nos.makedirs(model_dir, exist_ok=True)\n\nmodel_path = os.path.join(model_dir, \"xgb_v1b_multipattern.json\")\nmodel.save_model(model_path)\nprint(f\"Model saved to {model_path}\")\n\nmeta_path = os.path.join(model_dir, \"xgb_v1b_multipattern_meta.json\")\nwith open(meta_path, \"w\") as f:\n    json.dump({\n        \"features\": feature_cols,\n        \"n_features\": len(feature_cols),\n        \"train_size\": len(X_train),\n        \"test_size\": len(X_test),\n        \"class_balance\": {\"loss\": int(len(y) - y.sum()), \"win\": int(y.sum())},\n        \"scale_pos_weight\": float(spw),\n    }, f, indent=2)\nprint(f\"Feature metadata saved to {meta_path}\")\n\nprint(\"\\n=== V1b Training Complete ===\")\nprint(f\"Dataset:    {len(X)} rows ({len(X_train)} train / {len(X_test)} test)\")\nprint(f\"Features:   {len(feature_cols)}\")\nprint(f\"Accuracy:   {accuracy_score(y_test, y_pred):.3f} (baseline: {1 - y_test.mean():.3f})\")\nprint(f\"Precision:  {precision_score(y_test, y_pred, zero_division=0):.3f}\")\nprint(f\"Recall:     {recall_score(y_test, y_pred, zero_division=0):.3f}\")\nprint(f\"F1:         {f1_score(y_test, y_pred, zero_division=0):.3f}\")\nprint(f\"AUC-ROC:    {roc_auc_score(y_test, y_prob):.3f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}